# Google-Analytics


A former employer requested research regarding anonymous visitors to their website. I was to collect information on what visitors clicked, what they read, and what they visited all over multiple visits. I would not be able to change any code for the website. The end result would need to fit users into personas or audiences, and show the probability of a visitor performing a target action (submitting a resume or requesting to be contacted by a salesperson) given that they had already done other actions (read specific articles or clicked on certain elements). The delivered report would be used to guide content decisions by suggesting which articles were generating leads and inform hiring decisions based on the behavior of the applicant before applying. 

At first I was confident that this would not be possible. By design, Google Analytics doesn't give a persistent ID that can identify users over multiple visits. Google Analytics only gives a sample of aggregated data such as users per day. I would need data from many users over many visits in order to have enough data. Because I couldn't affect the code of the site, I couldn't add a login to identify a user or add a survey to gather additional information. This data would have to come completely from the Google Analytics API but it wasn't available in the format that I needed.

To collect the data I created a custom tag in Google Tag Manager (a product of Google Analytics) that would fire whenever a site visitor clicked on anything. Because Google Tag manager accepts custom values in the event label (an arbitrarily assigned value sent along with the tag) I set it to grab the GA tracking code from the cookies on the site, and concatenate it with the clicked dom element id. This gave me an key-value pair of user id and click event that I could extract later via the Google API. Because the event now is a unique text string, Google can't aggregate it when extracted from results. The query to fetch that data is a count of clicks per event label (which is always 1 per label). I've never seen anyone else try this, but it worked well and gave me row level data that would otherwise be aggregated. I then set similar events to track time on page and other features that I could use in my study. You can find some code examples of how to do this on my GitHub page.

I then processed the data to fit conventional machine learning models. Common open source machine learning libraries require a table of ids, features, and targets. I transformed the list of id-event tokens that I got from the API into a single table that, for each id, had a column for each event-feature marked as true or false. In the last column I put target variables that I would attempt to predict against later (applied for job x or requested introduction). Finally I loaded the table into a free-tier mySQL instance in AWS and set a job on my local machine to run the process as a daily job.




